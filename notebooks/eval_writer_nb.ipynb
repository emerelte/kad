{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import importlib\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(1, '/home/maciek/Documents/Magisterka/kubernetes-anomaly-detector')\n",
    "print(sys.path)\n",
    "\n",
    "\n",
    "import kad.visualization.visualization as visualization\n",
    "import kad.kad_utils.kad_utils as kad_utils\n",
    "import kad.models_evaluation.models_evaluator as models_evaluator\n",
    "import kad.model_selector.model_selector as model_selector\n",
    "from kad.model import i_model, autoencoder_model, hmm_model, lstm_model\n",
    "from kad.model import sarima_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "models_evaluator = importlib.reload(models_evaluator)\n",
    "\n",
    "kad_utils.customize_matplotlib()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "SARIMA_KEY: str = \"SARIMA\"\n",
    "AUTOENCODER_KEY: str = \"autoencoder\"\n",
    "HMM_KEY: str = \"HMM\"\n",
    "LSTM_KEY: str = \"LSTM\"\n",
    "\n",
    "ACC_KEY: str = \"accuracy\"\n",
    "VALID_ERR_KEY: str = \"valid_err\"\n",
    "PREC_KEY: str = \"precision\"\n",
    "RECALL_KEY: str = \"recall\"\n",
    "AUROC_KEY: str = \"auroc\"\n",
    "CUSTOM_KEY: str = \"custom\""
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "\n",
    "def downsampling_func(x: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Values column is replaced with a mean, and labels are replaced by True if any of the labels\n",
    "    in the original df was True\n",
    "    :param x: part of dataframe to squash to one row\n",
    "    :return: squashed pandas Series\n",
    "    \"\"\"\n",
    "    values = x[\"value\"].mean()\n",
    "\n",
    "    labels = (x[kad_utils.GROUND_TRUTH_COLUMN]).any()\n",
    "\n",
    "    return pd.Series([values, labels], index=[\"value\", kad_utils.GROUND_TRUTH_COLUMN])"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def perform_classical_evaluation(valid_err: float, evaluation_df: pd.DataFrame) -> dict:\n",
    "    visualization.visualize(evaluation_df, metric_name=\"value\")\n",
    "\n",
    "    evaluator = models_evaluator.ModelsEvaluator(df=evaluation_df)\n",
    "\n",
    "    print(\"Valid err: \", valid_err)\n",
    "    print(\"Accuracy: \", evaluator.get_accuracy())\n",
    "    print(\"Avg precision: \", evaluator.get_average_precision())\n",
    "    print(\"Recall: \", evaluator.get_recall_score())\n",
    "    print(\"AU ROC: \", evaluator.get_auroc())\n",
    "\n",
    "    return {ACC_KEY: evaluator.get_accuracy(),\n",
    "            # VALID_ERR_KEY: valid_err,\n",
    "            PREC_KEY: evaluator.get_average_precision(),\n",
    "            RECALL_KEY: evaluator.get_recall_score(),\n",
    "            AUROC_KEY: evaluator.get_auroc()}\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def perform_customized_evaluation(valid_err: float, evaluation_df: pd.DataFrame) -> dict:\n",
    "    visualization.visualize(evaluation_df, metric_name=\"value\")\n",
    "\n",
    "    evaluator = models_evaluator.ModelsEvaluator(df=evaluation_df)\n",
    "\n",
    "    print(\"Customized score: \", evaluator.get_customized_score())\n",
    "\n",
    "    return {CUSTOM_KEY: evaluator.get_customized_score()}\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "def evaluate_sarima(p_preprocessed_df: pd.DataFrame) -> Tuple[float, pd.DataFrame]:\n",
    "    values_df = p_preprocessed_df[[\"value\"]]\n",
    "    tsa = model_selector.ModelSelector(values_df)\n",
    "    dominant_freq = tsa.calculate_dominant_frequency()\n",
    "\n",
    "    model: i_model.IModel = sarima_model.SarimaModel(order=(0, 0, 0), seasonal_order=(1, 0, 1, dominant_freq))\n",
    "\n",
    "    train_df, test_df = train_test_split(values_df, shuffle=False, train_size=0.15)\n",
    "    valid_err = model.train(train_df)\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "    samples_to_predict = 18\n",
    "    for i in range(0, len(test_df), samples_to_predict):\n",
    "        results_df = model.test(test_df.iloc[i:i + samples_to_predict])\n",
    "\n",
    "    results_df[kad_utils.GROUND_TRUTH_COLUMN] = p_preprocessed_df[kad_utils.GROUND_TRUTH_COLUMN]\n",
    "    return valid_err, results_df.loc[test_df.index & results_df.index]"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def evaluate_autoencoder(p_preprocessed_df: pd.DataFrame) -> Tuple[float, pd.DataFrame]:\n",
    "    model: i_model.IModel = autoencoder_model.AutoEncoderModel(time_steps=12)\n",
    "\n",
    "    values_df = p_preprocessed_df[[\"value\"]]\n",
    "    train_df, test_df = train_test_split(values_df, shuffle=False, train_size=0.15)\n",
    "    valid_err = model.train(train_df)\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "    samples_to_predict = 60\n",
    "    for i in range(0, len(test_df), samples_to_predict):\n",
    "        if len(test_df.iloc[i:i + samples_to_predict]) < samples_to_predict:\n",
    "            break\n",
    "        results_df = model.test(test_df.iloc[i:i + samples_to_predict])\n",
    "\n",
    "    results_df[kad_utils.GROUND_TRUTH_COLUMN] = p_preprocessed_df[kad_utils.GROUND_TRUTH_COLUMN]\n",
    "    return valid_err, results_df.loc[test_df.index & results_df.index]"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "def evaluate_hmm(p_preprocessed_df: pd.DataFrame) -> Tuple[float, pd.DataFrame]:\n",
    "    model: i_model.IModel = hmm_model.HmmModel()\n",
    "\n",
    "    values_df = p_preprocessed_df[[\"value\"]]\n",
    "    train_df, test_df = train_test_split(values_df, shuffle=False, train_size=0.15)\n",
    "    valid_err = model.train(train_df)\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "    samples_to_predict = 5\n",
    "    for i in range(0, len(test_df), samples_to_predict):\n",
    "        if len(test_df.iloc[i:i + samples_to_predict]) < samples_to_predict:\n",
    "            break\n",
    "        results_df = model.test(test_df.iloc[i:i + samples_to_predict])\n",
    "\n",
    "    results_df[kad_utils.GROUND_TRUTH_COLUMN] = p_preprocessed_df[kad_utils.GROUND_TRUTH_COLUMN]\n",
    "    return valid_err, results_df.loc[test_df.index & results_df.index]"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def evaluate_lstm(p_preprocessed_df: pd.DataFrame) -> Tuple[float, pd.DataFrame]:\n",
    "    model: i_model.IModel = lstm_model.LstmModel(time_steps=12)\n",
    "\n",
    "    values_df = p_preprocessed_df[[\"value\"]]\n",
    "    train_df, test_df = train_test_split(values_df, shuffle=False, train_size=0.15)\n",
    "    valid_err = model.train(train_df)\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "    samples_to_predict = 25\n",
    "    for i in range(0, len(test_df), samples_to_predict):\n",
    "        if len(test_df.iloc[i:i + samples_to_predict]) < samples_to_predict:\n",
    "            break\n",
    "        results_df = model.test(test_df.iloc[i:i + samples_to_predict])\n",
    "\n",
    "    results_df[kad_utils.GROUND_TRUTH_COLUMN] = p_preprocessed_df[kad_utils.GROUND_TRUTH_COLUMN]\n",
    "    return valid_err, results_df.loc[test_df.index & results_df.index]"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "from typing import Tuple\n",
    "import concurrent\n",
    "\n",
    "eval_dict: dict = {}\n",
    "\n",
    "X_LABEL = \"timestamp\"\n",
    "data_dir = \"data/archive/\"\n",
    "file_dir = \"artificialWithAnomaly\"\n",
    "\n",
    "with open(\"data/archive/combined_windows.json\") as f:\n",
    "    ground_true_anomalies = json.load(f)\n",
    "\n",
    "executor = concurrent.futures.ProcessPoolExecutor()\n",
    "\n",
    "\n",
    "def task_executor(filename: str, original_df: pd.DataFrame, ground_true_anomalies: dict, eval_func):\n",
    "    file_eval_dict = {}\n",
    "\n",
    "    true_anomalies_ranges = ground_true_anomalies[\"/\".join(file_path.split(\"/\")[-2:])]\n",
    "    ground_true_anomalies_df = pd.DataFrame()\n",
    "    for anom_range in true_anomalies_ranges:\n",
    "        ground_true_anomalies_df = ground_true_anomalies_df.append(\n",
    "            original_df.loc[anom_range[0]:anom_range[1]])\n",
    "    original_df[kad_utils.GROUND_TRUTH_COLUMN] = [idx in ground_true_anomalies_df.index for idx in\n",
    "                                                  original_df.index.tolist()]\n",
    "\n",
    "    # SARIMA\n",
    "    preprocessed_df = original_df.groupby(pd.Grouper(freq=\"h\")).apply(downsampling_func)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    preprocessed_df[\"value\"] = scaler.fit_transform(preprocessed_df.values)\n",
    "\n",
    "    sarima_valid_err, sarima_eval_df = evaluate_sarima(preprocessed_df)\n",
    "    file_eval_dict[SARIMA_KEY] = eval_func(sarima_valid_err, sarima_eval_df)\n",
    "\n",
    "    # AUTOENCODER\n",
    "    preprocessed_df = original_df.copy()\n",
    "    preprocessed_df[\"value\"] = preprocessed_df[\"value\"].rolling(\"h\").sum()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    preprocessed_df[\"value\"] = scaler.fit_transform(preprocessed_df.values)\n",
    "\n",
    "    autoenc_valid_err, autoenc_eval_df = evaluate_autoencoder(preprocessed_df)\n",
    "    file_eval_dict[AUTOENCODER_KEY] = eval_func(autoenc_valid_err, autoenc_eval_df)\n",
    "\n",
    "    # HMM\n",
    "    preprocessed_df = original_df.groupby(pd.Grouper(freq=\"h\")).apply(downsampling_func)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    preprocessed_df[\"value\"] = scaler.fit_transform(preprocessed_df.values)\n",
    "\n",
    "    hmm_valid_err, hmm_eval_df = evaluate_hmm(preprocessed_df)\n",
    "    file_eval_dict[HMM_KEY] = eval_func(hmm_valid_err, hmm_eval_df)\n",
    "\n",
    "    # LSTM\n",
    "    preprocessed_df = original_df.groupby(pd.Grouper(freq=\"h\")).apply(downsampling_func)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    preprocessed_df[\"value\"] = scaler.fit_transform(preprocessed_df.values)\n",
    "\n",
    "    lstm_valid_err, lstm_eval_df = evaluate_lstm(preprocessed_df)\n",
    "    file_eval_dict[LSTM_KEY] = eval_func(lstm_valid_err, lstm_eval_df)\n",
    "\n",
    "    return filename, file_eval_dict\n",
    "\n",
    "\n",
    "files_to_eval = [\"art_daily_flatmiddle.csv\",\n",
    "                 \"art_daily_jumpsdown.csv\",\n",
    "                 \"art_daily_nojump.csv\",\n",
    "                 \"art_daily_jumpsup.csv\"]\n",
    "\n",
    "futures_table = list()\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        if filename in files_to_eval:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            original_df = pd.read_csv(file_path, parse_dates=True, index_col=\"timestamp\")\n",
    "            print(\"after reading csv\")\n",
    "            futures_table.append(\n",
    "                executor.submit(task_executor, filename, original_df, ground_true_anomalies, perform_customized_evaluation))\n",
    "\n",
    "for future in futures_table:\n",
    "    result: Tuple[str, dict] = future.result()\n",
    "    eval_dict[result[0]] = result[1]"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "eval_json = \"eval.json\"\n",
    "\n",
    "with open(eval_json, \"w\") as fp:\n",
    "    json.dump(eval_dict, fp)\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "language": "python",
   "name": "python38564bitvenv1ec9c175268d49259df14acff8f615bf"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}